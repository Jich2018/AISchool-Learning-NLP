{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('nlp': conda)",
   "display_name": "Python 3.7.6 64-bit ('nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2cf3fdb4b3807e391aeec544f52631bed41a9fbd637de42646e942e592730b80"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "\n",
    "train, test = imdb_dataset(train=True, test=True)\n",
    "#print(train[0])\n",
    "#print(test[0])\n",
    "\n",
    "train_x = [item['text'].lower() for item in train]\n",
    "train_y = [1 if (item['sentiment'] == 'pos') else 0 for item in train]\n",
    "#print(train_x[0])\n",
    "#print(train_y[0])\n",
    "\n",
    "test_x = [item['text'].lower() for item in test]\n",
    "test_y = [1 if (item['sentiment'] == 'pos') else 0 for item in test]\n",
    "#print(test_x[0])\n",
    "#print(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['bromwell', 'high', 'cartoon', 'comedy', 'ran', 'time', 'programs', 'school', 'life', 'teachers', '35', 'years', 'teaching', 'profession', 'lead', 'believe', 'bromwell', 'high', 's', 'satire', 'closer', 'reality', 'teachers', 'scramble', 'survive', 'financially', 'insightful', 'students', 'right', 'pathetic', 'teachers', 'pomp', 'pettiness', 'situation', 'remind', 'schools', 'knew', 'students', 'saw', 'episode', 'student', 'repeatedly', 'tried', 'burn', 'school', 'immediately', 'recalled', 'high', 'classic', 'line', 'inspector', 'i', 'm', 'sack', 'teachers', 'student', 'welcome', 'bromwell', 'high', 'expect', 'adults', 'age', 'think', 'bromwell', 'high', 'far', 'fetched', 'pity', 'isn', 't']\n['went', 'saw', 'movie', 'night', 'coaxed', 'friends', 'mine', 'i', 'll', 'admit', 'reluctant', 'knew', 'ashton', 'kutcher', 'able', 'comedy', 'wrong', 'kutcher', 'played', 'character', 'jake', 'fischer', 'well', 'kevin', 'costner', 'played', 'ben', 'randall', 'professionalism', 'sign', 'good', 'movie', 'toy', 'emotions', 'exactly', 'that', 'entire', 'theater', 'which', 'sold', 'out', 'overcome', 'laughter', 'half', 'movie', 'moved', 'tears', 'second', 'half', 'exiting', 'theater', 'saw', 'women', 'tears', 'grown', 'men', 'well', 'trying', 'desperately', 'let', 'crying', 'movie', 'great', 'suggest', 'judge']\n"
     ]
    }
   ],
   "source": [
    "# text preprocessing\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, strip_punctuation\n",
    "\n",
    "filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, strip_punctuation, strip_multiple_whitespaces]\n",
    "preprocessed_train_x = [preprocess_string(review, filters) for review in train_x]\n",
    "preprocessed_test_x = [preprocess_string(review, filters) for review in test_x]\n",
    "\n",
    "print(preprocessed_train_x[0])\n",
    "print(preprocessed_test_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[322, 1058, 208, 2097, 58, 391, 117, 151, 4939, 469]\n",
      "[422, 214, 16, 305, 351, 1869, 9, 938, 672, 491]\n"
     ]
    }
   ],
   "source": [
    "# only use the top_n_words for vocab since most of the words are too rare\n",
    "import numpy as np\n",
    "\n",
    "n_vocab = 5000\n",
    "max_review_length = 200\n",
    "\n",
    "with open(\"data/aclimdb/imdb.vocab\", 'r') as f:\n",
    "    vocab = f.read().splitlines()[:n_vocab]\n",
    "#print(vocab[:10])\n",
    "\n",
    "# convert data to index for embedding\n",
    "vocab_to_int = {word:i+1 for i, word in enumerate(vocab)}\n",
    "#print(list(vocab_to_int.items())[:10])\n",
    "\n",
    "int_to_vocab = {i:word for word, i in vocab_to_int.items()}\n",
    "#print(list(int_to_vocab.items())[:10])\n",
    "\n",
    "def get_encode(word):\n",
    "    if word in vocab_to_int:\n",
    "        return vocab_to_int[word]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "#print(preprocessed_train_x[0])\n",
    "encoded_train_x = [[get_encode(word)  for word in review] for review in preprocessed_train_x]\n",
    "#print(encoded_train_x[0][:10])\n",
    "encoded_train_x = [[word for word in review if (not np.isnan(word))][:max_review_length] for review in encoded_train_x]\n",
    "print(encoded_train_x[0][:10])\n",
    "\n",
    "#print(preprocessed_test_x[0])\n",
    "encoded_test_x = [[get_encode(word) for word in review] for review in preprocessed_test_x]\n",
    "#print(encoded_test_x[0][:10])\n",
    "encoded_test_x = [[word for word in review if (not np.isnan(word))][:max_review_length] for review in encoded_test_x]\n",
    "print(encoded_test_x[0][:10])\n",
    "\n",
    "\n",
    "# pad the data sequence\n",
    "def pad_text(encoded_reviews, seq_length):\n",
    "    reviews = []\n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "    return np.array(reviews)\n",
    "\n",
    "\n",
    "padded_train_x = pad_text(encoded_train_x, max_review_length)\n",
    "padded_test_x = pad_text(encoded_test_x, max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the dateset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import IntTensor\n",
    "import torch\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(padded_train_x, dtype=torch.int64), torch.tensor(train_y, dtype=torch.float32))\n",
    "test_data = TensorDataset(torch.tensor(padded_test_x, dtype=torch.int64), torch.tensor(test_y, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "from torch import nn\n",
    "\n",
    "n_embedding = 200 # embedding vector size\n",
    "n_hidden = 200\n",
    "n_layers = 1\n",
    "n_output = 1\n",
    "\n",
    "p_drop = 0.5\n",
    "\n",
    "class SentimentLstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab+1, n_embedding)\n",
    "        self.lstm = nn.LSTM(n_embedding, n_hidden, n_layers, batch_first=True, dropout=p_drop)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        #self.lstm = nn.LSTM(n_embedding, n_hidden, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, hiddens = self.lstm(embedded)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = outputs.contiguous().view(-1, n_hidden)\n",
    "        outputs = self.fc(outputs)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        outputs = outputs[:, -1]\n",
    "\n",
    "        return outputs, hiddens\n",
    "\n",
    "    #def init_hidden(self, batch_size):\n",
    "    #    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #    weights = next(self.parameters()).data\n",
    "    #    hiddens = (weights.new(n_layers, batch_size, n_hidden).zero_().to(device),\n",
    "    #         weights.new(n_layers, batch_size, n_hidden).zero_().to(device))\n",
    "        \n",
    "    #    return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 training Loss: 0.5361\n",
      "epoch 0 training Loss: 0.4672\n",
      "epoch 1 training Loss: 0.4409\n",
      "epoch 1 training Loss: 0.3587\n",
      "epoch 1 training Loss: 0.3271\n",
      "epoch 2 training Loss: 0.2503\n",
      "epoch 2 training Loss: 0.3008\n",
      "epoch 3 training Loss: 0.1975\n",
      "epoch 3 training Loss: 0.3376\n",
      "epoch 3 training Loss: 0.2737\n",
      "epoch 4 training Loss: 0.2175\n",
      "epoch 4 training Loss: 0.2234\n",
      "epoch 5 training Loss: 0.1027\n",
      "epoch 5 training Loss: 0.1206\n",
      "epoch 5 training Loss: 0.2138\n",
      "epoch 6 training Loss: 0.1273\n",
      "epoch 6 training Loss: 0.1266\n",
      "epoch 7 training Loss: 0.1109\n",
      "epoch 7 training Loss: 0.1162\n",
      "epoch 7 training Loss: 0.0821\n",
      "epoch 8 training Loss: 0.0603\n",
      "epoch 8 training Loss: 0.0756\n",
      "epoch 9 training Loss: 0.0639\n",
      "epoch 9 training Loss: 0.0714\n",
      "epoch 9 training Loss: 0.0863\n",
      "epoch 10 training Loss: 0.0378\n",
      "epoch 10 training Loss: 0.0308\n",
      "epoch 11 training Loss: 0.0159\n",
      "epoch 11 training Loss: 0.0649\n",
      "epoch 11 training Loss: 0.0382\n",
      "epoch 12 training Loss: 0.0084\n",
      "epoch 12 training Loss: 0.1202\n",
      "epoch 13 training Loss: 0.0088\n",
      "epoch 13 training Loss: 0.0322\n",
      "epoch 13 training Loss: 0.0322\n",
      "epoch 14 training Loss: 0.0055\n",
      "epoch 14 training Loss: 0.0170\n",
      "epoch 15 training Loss: 0.0156\n",
      "epoch 15 training Loss: 0.0431\n",
      "epoch 15 training Loss: 0.0380\n",
      "epoch 16 training Loss: 0.0887\n",
      "epoch 16 training Loss: 0.0032\n",
      "epoch 17 training Loss: 0.0060\n",
      "epoch 17 training Loss: 0.0096\n",
      "epoch 17 training Loss: 0.0359\n",
      "epoch 18 training Loss: 0.1165\n",
      "epoch 18 training Loss: 0.0147\n",
      "epoch 19 training Loss: 0.0536\n",
      "epoch 19 training Loss: 0.0107\n",
      "epoch 19 training Loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "# create and train the model\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "model = SentimentLstm()\n",
    "model.train()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "step = 0\n",
    "n_epoches = 20\n",
    "max_norm = 5\n",
    "\n",
    "for epoch in range(n_epoches):\n",
    "    #hiddens = model.init_hidden(batch_size)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        #hiddens = tuple([each.data for each in hiddens])\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, hiddens = model(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step % 100)  == 0:\n",
    "            print(\"epoch {} training Loss: {:.4f}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mdoel\n",
    "\n",
    "torch.save(model, \"lstm.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Loss: 0.8722\nTest Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, _ = model(inputs)\n",
    "    loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}